{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Sequence\n",
    "from dataclasses import dataclass\n",
    "from flask import Flask, request, jsonify\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from omegaconf import OmegaConf\n",
    "from dacite import from_dict\n",
    "from transformers import GPT2Tokenizer\n",
    "from Helpers.loaders import ModelSaverReader\n",
    "from xlstm.xlstm.utils import WeightDecayOptimGroupMixin\n",
    "from xlstm.xlstm.components.init import small_init_init_\n",
    "from xlstm.xlstm.xlstm_block_stack import xLSTMBlockStack, xLSTMBlockStackConfig\n",
    "\n",
    "# Load configuration\n",
    "cfg = OmegaConf.load('./params_app.yaml')\n",
    "\n",
    "@dataclass\n",
    "class xLSTMLMModelConfig(xLSTMBlockStackConfig):\n",
    "    vocab_size: int = -1\n",
    "    tie_weights: bool = True\n",
    "    weight_decay_on_embedding: bool = True\n",
    "    add_embedding_dropout: bool = True\n",
    "\n",
    "class xLSTMLMModel(WeightDecayOptimGroupMixin, nn.Module):\n",
    "    config_class = xLSTMLMModelConfig\n",
    "\n",
    "    def __init__(self, config: xLSTMLMModelConfig, **kwargs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.xlstm_block_stack = xLSTMBlockStack(config=config)\n",
    "        self.token_embedding = nn.Embedding(num_embeddings=config.vocab_size, embedding_dim=config.embedding_dim)\n",
    "        self.emb_dropout = nn.Dropout(config.dropout) if config.add_embedding_dropout else nn.Identity()\n",
    "\n",
    "        self.lm_head = nn.Linear(\n",
    "            in_features=config.embedding_dim,\n",
    "            out_features=config.vocab_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        if config.tie_weights:\n",
    "            self.lm_head.weight = self.token_embedding.weight\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.xlstm_block_stack.reset_parameters()\n",
    "\n",
    "        small_init_init_(self.token_embedding.weight, dim=self.config.embedding_dim)\n",
    "\n",
    "        if not self.config.tie_weights:\n",
    "            small_init_init_(self.lm_head.weight, dim=self.config.embedding_dim)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.token_embedding(idx)\n",
    "        x = self.emb_dropout(x)\n",
    "        x = self.xlstm_block_stack(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    def step(\n",
    "        self, idx: torch.Tensor, state: dict[str, dict[str, tuple[torch.Tensor, ...]]] = None, **kwargs\n",
    "    ) -> tuple[torch.Tensor, dict[str, dict[str, tuple[torch.Tensor, ...]]]]:\n",
    "        x = self.token_embedding(idx)\n",
    "        x = self.emb_dropout(x)\n",
    "        x, state = self.xlstm_block_stack.step(x, state=state, **kwargs)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits, state\n",
    "\n",
    "    def _create_weight_decay_optim_groups(self, **kwargs) -> tuple[Sequence[nn.Parameter], Sequence[nn.Parameter]]:\n",
    "        weight_decay, no_weight_decay = super()._create_weight_decay_optim_groups(**kwargs)\n",
    "        # remove token embedding and add it to the correct group, according to the config\n",
    "        weight_decay = list(weight_decay)\n",
    "        removed = 0\n",
    "        for idx in range(len(weight_decay)):\n",
    "            if weight_decay[idx - removed] is self.token_embedding.weight:\n",
    "                weight_decay.pop(idx - removed)\n",
    "                removed += 1\n",
    "        weight_decay = tuple(weight_decay)\n",
    "        if self.config.weight_decay_on_embedding:\n",
    "            weight_decay += (self.token_embedding.weight,)\n",
    "        else:\n",
    "            no_weight_decay += (self.token_embedding.weight,)\n",
    "\n",
    "        return weight_decay, no_weight_decay\n",
    "    \n",
    "# Define binary classification model\n",
    "class xLSTMLMModelBinary(nn.Module):\n",
    "    def __init__(self, config, pretrained_model=None):\n",
    "        super(xLSTMLMModelBinary, self).__init__()\n",
    "        if pretrained_model is None:\n",
    "            self.pretrained_model = xLSTMLMModel(config)\n",
    "        else:\n",
    "            self.pretrained_model = pretrained_model\n",
    "        self.fc = nn.Linear(config.embedding_dim, 1)  # Assuming embedding_dim matches hidden state size\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        hidden_states = self.pretrained_model.token_embedding(input_ids)\n",
    "        hidden_states = self.pretrained_model.xlstm_block_stack(hidden_states)\n",
    "        pooled_outputs = hidden_states.mean(dim=1)  # Pooling over the context length dimension\n",
    "        pooled_outputs = self.dropout(pooled_outputs)\n",
    "        logits = self.fc(pooled_outputs)\n",
    "        return logits.squeeze(-1)\n",
    "\n",
    "\n",
    "# Load the pre-trained weights into the binary classification model\n",
    "def load_pretrained_weights(pretrained_model, binary_model):\n",
    "    pretrained_dict = pretrained_model.state_dict()\n",
    "    model_dict = binary_model.state_dict()\n",
    "    # 1. Filter out unnecessary keys\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and 'fc' not in k}\n",
    "    # 2. Overwrite entries in the existing state dict\n",
    "    model_dict.update(pretrained_dict)\n",
    "    # 3. Load the new state dict\n",
    "    binary_model.load_state_dict(model_dict)\n",
    "\n",
    "    return binary_model\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "class HateSpeechDetector:\n",
    "    def __init__(self, model, tokenizer, context_length, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.context_length = context_length\n",
    "        self.device = device\n",
    "\n",
    "    def predict(self, tweet):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Tokenize tweet\n",
    "            inputs = self.tokenizer.encode_plus(\n",
    "                tweet,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.context_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids = inputs['input_ids'].to(self.device)\n",
    "            # Perform classification\n",
    "            print(input_ids.shape)\n",
    "            outputs = self.model(input_ids)\n",
    "            prediction = torch.sigmoid(outputs).item()\n",
    "            is_hate = prediction >= 0.5  # Adjust threshold if needed\n",
    "            return is_hate\n",
    "\n",
    "    def display_prediction(self, tweet):\n",
    "        is_hate = self.predict(tweet)\n",
    "        color = 'red' if is_hate else 'green'\n",
    "        label = 'Hate' if is_hate else 'Not Hate'\n",
    "        result_html = f'<span style=\"color:{color}; font-weight:bold;\">{label}</span>'\n",
    "        display(HTML(f\"<p>{tweet}</p><p>{result_html}</p>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./Models/model1_bin_final.pth\n",
      "torch.Size([1, 128])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p>suck my dick, stupid leftie! </p><p><span style=\"color:red; font-weight:bold;\">Hate</span></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Access the schedul dictionary directly\n",
    "schedul = {\n",
    "    1: cfg.model.schedul['first'],\n",
    "    int(cfg.training.num_steps * (1/8)): cfg.model.schedul['quarter'],\n",
    "    int(cfg.training.num_steps * (1/4)): cfg.model.schedul['half'],\n",
    "    int(cfg.training.num_steps * (1/2)): cfg.model.schedul['three_quarters']\n",
    "}\n",
    "\n",
    "# Ensure we use the final context length\n",
    "final_context_length = schedul[max(schedul.keys())]\n",
    "cfg.model.context_length = final_context_length\n",
    "\n",
    "from dacite import from_dict\n",
    "\n",
    "model_saver_reader = ModelSaverReader('./Models')\n",
    "model_bin_final_10k = model_saver_reader.load_model(xLSTMLMModelBinary, f\"model1_bin_final\", from_dict(xLSTMLMModelConfig, OmegaConf.to_container(cfg.model, resolve=True))).to(cfg.training.device)\n",
    "model_bin_final_10k.eval()\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "class HateSpeechDetector:\n",
    "    def __init__(self, model, tokenizer, context_length, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.context_length = context_length\n",
    "        self.device = device\n",
    "\n",
    "    def predict(self, tweet):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Tokenize tweet\n",
    "            inputs = self.tokenizer.encode_plus(\n",
    "                tweet,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.context_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids = inputs['input_ids'].to(self.device)\n",
    "            # Perform classification\n",
    "            print(input_ids.shape)\n",
    "            outputs = self.model(input_ids)\n",
    "            prediction = torch.sigmoid(outputs).item()\n",
    "            is_hate = prediction >= 0.5  # Adjust threshold if needed\n",
    "            return is_hate\n",
    "\n",
    "    def display_prediction(self, tweet):\n",
    "        is_hate = self.predict(tweet)\n",
    "        color = 'red' if is_hate else 'green'\n",
    "        label = 'Hate' if is_hate else 'Not Hate'\n",
    "        result_html = f'<span style=\"color:{color}; font-weight:bold;\">{label}</span>'\n",
    "        display(HTML(f\"<p>{tweet}</p><p>{result_html}</p>\"))\n",
    "\n",
    "\n",
    "# Load configuration\n",
    "#cfg = OmegaConf.load('/content/drive/MyDrive/Hate/parity_xlstm11.yaml')\n",
    "cfg = OmegaConf.load('./params_app.yaml')\n",
    "\n",
    "# Provide default value if cfg.training.val_every_step is not defined\n",
    "if cfg.training.val_every_step is None:\n",
    "    cfg.training.val_every_step = 100  # Set to 100 or any reasonable default value\n",
    "\n",
    "# Access the schedul dictionary directly\n",
    "schedul = {\n",
    "    1: cfg.model.schedul['first'],\n",
    "    int(cfg.training.num_steps * (1/8)): cfg.model.schedul['quarter'],\n",
    "    int(cfg.training.num_steps * (1/4)): cfg.model.schedul['half'],\n",
    "    int(cfg.training.num_steps * (1/2)): cfg.model.schedul['three_quarters']\n",
    "}\n",
    "\n",
    "# Ensure we use the final context length\n",
    "final_context_length = schedul[max(schedul.keys())]\n",
    "cfg.model.context_length = final_context_length\n",
    "\n",
    "# Initialize the detector\n",
    "detector = HateSpeechDetector(model_bin_final_10k, tokenizer, cfg.model.context_length, cfg.training.device)\n",
    "\n",
    "# Example prediction\n",
    "tweet = \"you like New York!\"\n",
    "#tweet = \"fuck you kill idiot\"\n",
    "#tweet = \"suck your vagina\"\n",
    "#tweet = \"suck your lolipop\"\n",
    "tweet = \"rape you stupid bitch\"\n",
    "#tweet = \"I have the\"\n",
    "#tweet = \"lovely woman who respect animals and others\"\n",
    "tweet = \"bitch who loves animals\"\n",
    "tweet = \"suck my dick, stupid leftie! \"\n",
    "detector.display_prediction(tweet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lucy",
   "language": "python",
   "name": "lucy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
